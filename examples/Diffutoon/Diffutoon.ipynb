{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maksbelous/DiffSynth-Studio/blob/first_test/examples/Diffutoon/Diffutoon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ObdI5jCB8xy"
      },
      "source": [
        "# DiffSynth Studio\n",
        "\n",
        "Welcome to DiffSynth Studio! This is an example of Diffutoon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSkKX7O2BwuM"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msCpt0pLnT8W",
        "outputId": "50423675-6b50-4b24-e008-466853b07435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DiffSynth-Studio' already exists and is not an empty directory.\n",
            "/content/DiffSynth-Studio\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Artiprocher/DiffSynth-Studio.git\n",
        "!pip install -q transformers controlnet-aux==0.0.7 streamlit streamlit-drawable-canvas imageio imageio[ffmpeg] safetensors einops cupy-cuda12x\n",
        "%cd /content/DiffSynth-Studio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eCu_rlKB3kK"
      },
      "source": [
        "## Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9znMkpVj3qZ1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "def download_model(url, file_path):\n",
        "  model_file = requests.get(url, allow_redirects=True)\n",
        "  with open(file_path, \"wb\") as f:\n",
        "    f.write(model_file.content)\n",
        "\n",
        "download_model(\"https://civitai.com/api/download/models/229575\", \"models/stable_diffusion/aingdiffusion_v12.safetensors\")\n",
        "download_model(\"https://huggingface.co/guoyww/animatediff/resolve/main/mm_sd_v15_v2.ckpt\", \"models/AnimateDiff/mm_sd_v15_v2.ckpt\")\n",
        "download_model(\"https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_lineart.pth\", \"models/ControlNet/control_v11p_sd15_lineart.pth\")\n",
        "download_model(\"https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1e_sd15_tile.pth\", \"models/ControlNet/control_v11f1e_sd15_tile.pth\")\n",
        "download_model(\"https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth\", \"models/ControlNet/control_v11f1p_sd15_depth.pth\")\n",
        "download_model(\"https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_softedge.pth\", \"models/ControlNet/control_v11p_sd15_softedge.pth\")\n",
        "download_model(\"https://huggingface.co/lllyasviel/Annotators/resolve/main/dpt_hybrid-midas-501f0c75.pt\", \"models/Annotators/dpt_hybrid-midas-501f0c75.pt\")\n",
        "download_model(\"https://huggingface.co/lllyasviel/Annotators/resolve/main/ControlNetHED.pth\", \"models/Annotators/ControlNetHED.pth\")\n",
        "download_model(\"https://huggingface.co/lllyasviel/Annotators/resolve/main/sk_model.pth\", \"models/Annotators/sk_model.pth\")\n",
        "download_model(\"https://huggingface.co/lllyasviel/Annotators/resolve/main/sk_model2.pth\", \"models/Annotators/sk_model2.pth\")\n",
        "download_model(\"https://civitai.com/api/download/models/25820?type=Model&format=PickleTensor&size=full&fp=fp16\", \"models/textual_inversion/verybadimagenegative_v1.3.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwOq2lWtKVYS"
      },
      "source": [
        "## Run Diffutoon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tII_XRY-PJeo"
      },
      "source": [
        "### Config Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vsd2alA3PrGe"
      },
      "outputs": [],
      "source": [
        "config_stage_1_template = {\n",
        "    \"models\": {\n",
        "        \"model_list\": [\n",
        "            \"models/stable_diffusion/aingdiffusion_v12.safetensors\",\n",
        "            \"models/ControlNet/control_v11p_sd15_softedge.pth\",\n",
        "            \"models/ControlNet/control_v11f1p_sd15_depth.pth\"\n",
        "        ],\n",
        "        \"textual_inversion_folder\": \"models/textual_inversion\",\n",
        "        \"device\": \"cuda\",\n",
        "        \"lora_alphas\": [],\n",
        "        \"controlnet_units\": [\n",
        "            {\n",
        "                \"processor_id\": \"softedge\",\n",
        "                \"model_path\": \"models/ControlNet/control_v11p_sd15_softedge.pth\",\n",
        "                \"scale\": 0.5\n",
        "            },\n",
        "            {\n",
        "                \"processor_id\": \"depth\",\n",
        "                \"model_path\": \"models/ControlNet/control_v11f1p_sd15_depth.pth\",\n",
        "                \"scale\": 0.5\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"input_frames\": {\n",
        "            \"video_file\": \"/content/input_video.mp4\",\n",
        "            \"image_folder\": None,\n",
        "            \"height\": 512,\n",
        "            \"width\": 512,\n",
        "            \"start_frame_id\": 0,\n",
        "            \"end_frame_id\": 30\n",
        "        },\n",
        "        \"controlnet_frames\": [\n",
        "            {\n",
        "                \"video_file\": \"/content/input_video.mp4\",\n",
        "                \"image_folder\": None,\n",
        "                \"height\": 512,\n",
        "                \"width\": 512,\n",
        "                \"start_frame_id\": 0,\n",
        "                \"end_frame_id\": 30\n",
        "            },\n",
        "            {\n",
        "                \"video_file\": \"/content/input_video.mp4\",\n",
        "                \"image_folder\": None,\n",
        "                \"height\": 512,\n",
        "                \"width\": 512,\n",
        "                \"start_frame_id\": 0,\n",
        "                \"end_frame_id\": 30\n",
        "            }\n",
        "        ],\n",
        "        \"output_folder\": \"data/examples/diffutoon_edit/color_video\",\n",
        "        \"fps\": 25\n",
        "    },\n",
        "    \"smoother_configs\": [\n",
        "        {\n",
        "            \"processor_type\": \"FastBlend\",\n",
        "            \"config\": {}\n",
        "        }\n",
        "    ],\n",
        "    \"pipeline\": {\n",
        "        \"seed\": 0,\n",
        "        \"pipeline_inputs\": {\n",
        "            \"prompt\": \"best quality, perfect anime illustration, orange clothes, night, a girl is dancing, smile, solo, black silk stockings\",\n",
        "            \"negative_prompt\": \"verybadimagenegative_v1.3\",\n",
        "            \"cfg_scale\": 7.0,\n",
        "            \"clip_skip\": 1,\n",
        "            \"denoising_strength\": 0.9,\n",
        "            \"num_inference_steps\": 20,\n",
        "            \"animatediff_batch_size\": 8,\n",
        "            \"animatediff_stride\": 4,\n",
        "            \"unet_batch_size\": 8,\n",
        "            \"controlnet_batch_size\": 8,\n",
        "            \"cross_frame_attention\": True,\n",
        "            \"smoother_progress_ids\": [-1],\n",
        "            # The following parameters will be overwritten. You don't need to modify them.\n",
        "            \"input_frames\": [],\n",
        "            \"num_frames\": 30,\n",
        "            \"width\": 512,\n",
        "            \"height\": 512,\n",
        "            \"controlnet_frames\": []\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "config_stage_2_template = {\n",
        "    \"models\": {\n",
        "        \"model_list\": [\n",
        "            \"models/stable_diffusion/aingdiffusion_v12.safetensors\",\n",
        "            \"models/AnimateDiff/mm_sd_v15_v2.ckpt\",\n",
        "            \"models/ControlNet/control_v11f1e_sd15_tile.pth\",\n",
        "            \"models/ControlNet/control_v11p_sd15_lineart.pth\"\n",
        "        ],\n",
        "        \"textual_inversion_folder\": \"models/textual_inversion\",\n",
        "        \"device\": \"cuda\",\n",
        "        \"lora_alphas\": [],\n",
        "        \"controlnet_units\": [\n",
        "            {\n",
        "                \"processor_id\": \"tile\",\n",
        "                \"model_path\": \"models/ControlNet/control_v11f1e_sd15_tile.pth\",\n",
        "                \"scale\": 0.5\n",
        "            },\n",
        "            {\n",
        "                \"processor_id\": \"lineart\",\n",
        "                \"model_path\": \"models/ControlNet/control_v11p_sd15_lineart.pth\",\n",
        "                \"scale\": 0.5\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"input_frames\": {\n",
        "            \"video_file\": \"/content/input_video.mp4\",\n",
        "            \"image_folder\": None,\n",
        "            \"height\": 1280,\n",
        "            \"width\": 720,\n",
        "            \"start_frame_id\": 0,\n",
        "            \"end_frame_id\": 60\n",
        "        },\n",
        "        \"controlnet_frames\": [\n",
        "            {\n",
        "                \"video_file\": \"/content/input_video.mp4\",\n",
        "                \"image_folder\": None,\n",
        "                \"height\": 1280,\n",
        "                \"width\": 720,\n",
        "                \"start_frame_id\": 0,\n",
        "                \"end_frame_id\": 60\n",
        "            },\n",
        "            {\n",
        "                \"video_file\": \"/content/input_video.mp4\",\n",
        "                \"image_folder\": None,\n",
        "                \"height\": 1280,\n",
        "                \"width\": 720,\n",
        "                \"start_frame_id\": 0,\n",
        "                \"end_frame_id\": 60\n",
        "            }\n",
        "        ],\n",
        "        \"output_folder\": \"/content/output\",\n",
        "        \"fps\": 25\n",
        "    },\n",
        "    \"pipeline\": {\n",
        "        \"seed\": 0,\n",
        "        \"pipeline_inputs\": {\n",
        "            \"prompt\": \"best quality, perfect anime illustration, light, a girl is dancing, smile, solo\",\n",
        "            \"negative_prompt\": \"verybadimagenegative_v1.3\",\n",
        "            \"cfg_scale\": 7.0,\n",
        "            \"clip_skip\": 2,\n",
        "            \"denoising_strength\": 1.0,\n",
        "            \"num_inference_steps\": 10,\n",
        "            \"animatediff_batch_size\": 16,\n",
        "            \"animatediff_stride\": 8,\n",
        "            \"unet_batch_size\": 1,\n",
        "            \"controlnet_batch_size\": 1,\n",
        "            \"cross_frame_attention\": False,\n",
        "            # The following parameters will be overwritten. You don't need to modify them.\n",
        "            \"input_frames\": [],\n",
        "            \"num_frames\": 30,\n",
        "            \"width\": 1536,\n",
        "            \"height\": 1536,\n",
        "            \"controlnet_frames\": []\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "113QAmNHP6T_"
      },
      "source": [
        "### Upload Input Video\n",
        "\n",
        "Before you run the following code, please upload your input video to `/content/input_video.mp4`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyqAsj1o5U9B"
      },
      "source": [
        "### Toon Shading\n",
        "\n",
        "Render your video in an anime style.\n",
        "\n",
        "We highly recommend you to use a higher resolution for better visual quality. The default resolution of Diffutoon is 1536x1536, which requires 22GB VRAM. If you don't have enough VRAM, 1024x1024 is also acceptable.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modelscope"
      ],
      "metadata": {
        "id": "M3b6foHQS8Sy",
        "outputId": "7746fe7b-444f-424f-9727-b7df2cc8ef7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting modelscope\n",
            "  Downloading modelscope-1.19.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.25 in /usr/local/lib/python3.10/dist-packages (from modelscope) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from modelscope) (4.66.6)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from modelscope) (2.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (2024.8.30)\n",
            "Downloading modelscope-1.19.2-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: modelscope\n",
            "Successfully installed modelscope-1.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "761nbrgeKMvj",
        "outputId": "76a7d9aa-dd4d-4eed-e336-32bbe9e962e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:diffsynth.models.kolors_text_encoder:Failed to load cpm_kernels:No module named 'cpm_kernels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models from: models/stable_diffusion/aingdiffusion_v12.safetensors\n",
            "    model_name: sd_text_encoder model_class: SDTextEncoder\n",
            "    model_name: sd_unet model_class: SDUNet\n",
            "    model_name: sd_vae_decoder model_class: SDVAEDecoder\n",
            "    model_name: sd_vae_encoder model_class: SDVAEEncoder\n",
            "    The following models are loaded: ['sd_text_encoder', 'sd_unet', 'sd_vae_decoder', 'sd_vae_encoder'].\n",
            "Loading models from: models/AnimateDiff/mm_sd_v15_v2.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/DiffSynth-Studio/diffsynth/models/utils.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(file_path, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    model_name: sd_motion_modules model_class: SDMotionModel\n",
            "    The following models are loaded: ['sd_motion_modules'].\n",
            "Loading models from: models/ControlNet/control_v11f1e_sd15_tile.pth\n",
            "    model_name: sd_controlnet model_class: SDControlNet\n",
            "    The following models are loaded: ['sd_controlnet'].\n",
            "Loading models from: models/ControlNet/control_v11p_sd15_lineart.pth\n",
            "    model_name: sd_controlnet model_class: SDControlNet\n",
            "    The following models are loaded: ['sd_controlnet'].\n",
            "Using sd_text_encoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_unet from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_vae_decoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_vae_encoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_controlnet from models/ControlNet/control_v11f1e_sd15_tile.pth.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/lineart/__init__.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/lineart/__init__.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  coarse_model.load_state_dict(torch.load(coarse_model_path, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using sd_controlnet from models/ControlNet/control_v11p_sd15_lineart.pth.\n",
            "No sd_ipadapter models available.\n",
            "No sd_ipadapter_clip_image_encoder models available.\n",
            "Using sd_motion_modules from models/AnimateDiff/mm_sd_v15_v2.ckpt.\n",
            "Textual inversion verybadimagenegative_v1.3 is enabled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 181.58it/s]\n",
            "100%|██████████| 60/60 [00:07<00:00,  7.63it/s]\n",
            "100%|██████████| 10/10 [13:23<00:00, 80.34s/it]\n",
            "Saving images: 100%|██████████| 60/60 [00:34<00:00,  1.74it/s]\n",
            "Saving video: 100%|██████████| 60/60 [00:00<00:00, 67.41it/s]\n"
          ]
        }
      ],
      "source": [
        "from diffsynth import SDVideoPipelineRunner\n",
        "\n",
        "\n",
        "config = config_stage_2_template.copy()\n",
        "config[\"data\"][\"input_frames\"] = {\n",
        "    \"video_file\": \"/content/input_video.mp4\",\n",
        "    \"image_folder\": None,\n",
        "    \"height\": 1280,\n",
        "    \"width\": 720,\n",
        "    \"start_frame_id\": 0,\n",
        "    \"end_frame_id\": 60\n",
        "}\n",
        "config[\"data\"][\"controlnet_frames\"] = [config[\"data\"][\"input_frames\"], config[\"data\"][\"input_frames\"]]\n",
        "config[\"data\"][\"output_folder\"] = \"/content/toon_video\"\n",
        "config[\"data\"][\"fps\"] = 25\n",
        "\n",
        "runner = SDVideoPipelineRunner()\n",
        "runner.run(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wujhGUmDIwY"
      },
      "source": [
        "Let's see the video!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TBNAigacAq6h"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(\"/content/toon_video/video.mp4\", \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "<source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48hQfX--5YGi"
      },
      "source": [
        "### Toon Shading with Editing Signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAQ9Zq-3-MH6"
      },
      "source": [
        "In stage 1, input your prompt, and diffutoon will generate the editing signals in the format of low-resolution color video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtDzYgIq5bgg"
      },
      "outputs": [],
      "source": [
        "from diffsynth import SDVideoPipelineRunner\n",
        "\n",
        "\n",
        "config_stage_1 = config_stage_1_template.copy()\n",
        "config_stage_1[\"data\"][\"input_frames\"] = {\n",
        "    \"video_file\": \"/content/input_video.mp4\",\n",
        "    \"image_folder\": None,\n",
        "    \"height\": 512,\n",
        "    \"width\": 512,\n",
        "    \"start_frame_id\": 0,\n",
        "    \"end_frame_id\": 30\n",
        "}\n",
        "config_stage_1[\"data\"][\"controlnet_frames\"] = [config_stage_1[\"data\"][\"input_frames\"], config_stage_1[\"data\"][\"input_frames\"]]\n",
        "config_stage_1[\"data\"][\"output_folder\"] = \"/content/color_video\"\n",
        "config_stage_1[\"data\"][\"fps\"] = 25\n",
        "config_stage_1[\"pipeline\"][\"pipeline_inputs\"][\"prompt\"] = \"best quality, perfect anime illustration, orange clothes, night, a girl is dancing, smile, solo, black silk stockings\"\n",
        "\n",
        "runner = SDVideoPipelineRunner()\n",
        "runner.run(config_stage_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9_AWwhi-pA9"
      },
      "source": [
        "In stage 2, diffutoon will rerender the whole video according to the editing signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFysCk7y51i_"
      },
      "outputs": [],
      "source": [
        "from diffsynth import SDVideoPipelineRunner\n",
        "\n",
        "\n",
        "config_stage_2 = config_stage_2_template.copy()\n",
        "config_stage_2[\"data\"][\"input_frames\"] = {\n",
        "    \"video_file\": \"/content/input_video.mp4\",\n",
        "    \"image_folder\": None,\n",
        "    \"height\": 1024,\n",
        "    \"width\": 1024,\n",
        "    \"start_frame_id\": 0,\n",
        "    \"end_frame_id\": 30\n",
        "}\n",
        "config_stage_2[\"data\"][\"controlnet_frames\"][0] = {\n",
        "    \"video_file\": \"/content/color_video/video.mp4\",\n",
        "    \"image_folder\": None,\n",
        "    \"height\": config_stage_2[\"data\"][\"input_frames\"][\"height\"],\n",
        "    \"width\": config_stage_2[\"data\"][\"input_frames\"][\"width\"],\n",
        "    \"start_frame_id\": None,\n",
        "    \"end_frame_id\": None\n",
        "}\n",
        "config_stage_2[\"data\"][\"controlnet_frames\"][1] = config[\"data\"][\"input_frames\"]\n",
        "config_stage_2[\"data\"][\"output_folder\"] = \"/content/edit_video\"\n",
        "config_stage_2[\"data\"][\"fps\"] = 25\n",
        "\n",
        "runner = SDVideoPipelineRunner()\n",
        "runner.run(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIPrCAIS_Im0"
      },
      "source": [
        "Let's see the video!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2nz7rew-7VI"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(\"/content/edit_video/video.mp4\", \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "<source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tII_XRY-PJeo"
      ],
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}